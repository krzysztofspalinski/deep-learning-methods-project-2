{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class ResnetIdentityBlock(tf.keras.Model):\n",
    "  def __init__(self, kernel_size, filters, batch_normalization=True, conv_first=False):\n",
    "    super(ResnetIdentityBlock, self).__init__(name='')\n",
    "    \n",
    "    self.residual_layers = []\n",
    "    \n",
    "    for i in range(len(filters)):\n",
    "        \n",
    "        if conv_first:\n",
    "            setattr(self, 'conv' + str(i+1), tf.keras.layers.Conv2D(filters[i], kernel_size, padding='same'))\n",
    "            self.residual_layers.append('conv' + str(i+1))\n",
    "\n",
    "            if batch_normalization:\n",
    "                setattr(self, 'bn' + str(i+1), tf.keras.layers.BatchNormalization())\n",
    "                self.residual_layers.append('bn' + str(i+1))\n",
    "        \n",
    "        else:\n",
    "            if batch_normalization:\n",
    "                setattr(self, 'bn' + str(i+1), tf.keras.layers.BatchNormalization())\n",
    "                self.residual_layers.append('bn' + str(i+1))\n",
    "            \n",
    "            setattr(self, 'conv' + str(i+1), tf.keras.layers.Conv2D(filters[i], kernel_size, padding='same'))\n",
    "            self.residual_layers.append('conv' + str(i+1))\n",
    "\n",
    "            \n",
    "            \n",
    "  def call(self, input_tensor, training=False):\n",
    "    \n",
    "    x = input_tensor\n",
    "    \n",
    "    for layer in self.residual_layers:\n",
    "        if isinstance(layer, tf.keras.layers.Conv2D):\n",
    "            x = getattr(self, layer)(x)\n",
    "        else: \n",
    "            x = getattr(self, layer)(x, training=False)\n",
    "        x = tf.nn.relu(x)\n",
    "        \n",
    "    x += input_tensor\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "\n",
    "(x_train,y_train),(x_test,y_test) = cifar10.load_data() \n",
    "\n",
    "x_train = x_train /255\n",
    "x_test = x_test / 255\n",
    "\n",
    "\n",
    "y_train_orig = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import json\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "        # set input mean to 0 over the dataset\n",
    "        featurewise_center=False,\n",
    "        # set each sample mean to 0\n",
    "        samplewise_center=False,\n",
    "        # divide inputs by std of dataset\n",
    "        featurewise_std_normalization=False,\n",
    "        # divide each input by its std\n",
    "        samplewise_std_normalization=False,\n",
    "        # apply ZCA whitening\n",
    "        zca_whitening=False,\n",
    "        # epsilon for ZCA whitening\n",
    "        zca_epsilon=1e-06,\n",
    "        # randomly rotate images in the range (deg 0 to 180)\n",
    "        rotation_range=15,                                       #5,\n",
    "        # randomly shift images horizontally\n",
    "        width_shift_range=0.1,\n",
    "        # randomly shift images vertically\n",
    "        height_shift_range=0.1,\n",
    "        # set range for random shear\n",
    "        shear_range=0.,\n",
    "        # set range for random zoom\n",
    "        zoom_range=0.,\n",
    "        # set range for random channel shifts\n",
    "        channel_shift_range=0.,\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        # value used for fill_mode = \"constant\"\n",
    "        cval=0.,\n",
    "        # randomly flip images\n",
    "        horizontal_flip=True,\n",
    "        # randomly flip images\n",
    "        vertical_flip=False,\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=None,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # image data format, either \"channels_first\" or \"channels_last\"\n",
    "        data_format=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "NUM_CLASSES = 10\n",
    "INPUT_SHAPE = (32, 32, 3)\n",
    "\n",
    "NUM_EPOCHS = 100\n",
    "learning_rate = 1e-5\n",
    "BATCH_SIZE=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 30, 30, 512)       14336     \n",
      "_________________________________________________________________\n",
      "resnet_identity_block (Resne (None, 30, 30, 512)       4723712   \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 30, 30, 256)       1179904   \n",
      "_________________________________________________________________\n",
      "resnet_identity_block_1 (Res (None, 30, 30, 256)       1182208   \n",
      "_________________________________________________________________\n",
      "resnet_identity_block_2 (Res (None, 30, 30, 256)       1182208   \n",
      "_________________________________________________________________\n",
      "resnet_identity_block_3 (Res (None, 30, 30, 256)       1182208   \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 30, 30, 128)       295040    \n",
      "_________________________________________________________________\n",
      "resnet_identity_block_4 (Res (None, 30, 30, 128)       296192    \n",
      "_________________________________________________________________\n",
      "resnet_identity_block_5 (Res (None, 30, 30, 128)       296192    \n",
      "_________________________________________________________________\n",
      "resnet_identity_block_6 (Res (None, 30, 30, 128)       296192    \n",
      "_________________________________________________________________\n",
      "conv2d_17 (Conv2D)           (None, 30, 30, 64)        73792     \n",
      "_________________________________________________________________\n",
      "resnet_identity_block_7 (Res (None, 30, 30, 64)        33408     \n",
      "_________________________________________________________________\n",
      "resnet_identity_block_8 (Res (None, 30, 30, 64)        33408     \n",
      "_________________________________________________________________\n",
      "resnet_identity_block_9 (Res (None, 30, 30, 64)        33408     \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 30, 30, 64)        256       \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 30, 30, 64)        0         \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 3, 3, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               73856     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 10,897,610\n",
      "Trainable params: 10,890,058\n",
      "Non-trainable params: 7,552\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Architecture\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Conv2D(512, (3, 3), activation='relu', input_shape=INPUT_SHAPE))\n",
    "\n",
    "model.add(ResnetIdentityBlock((3,3), filters=(512, 512)))\n",
    "model.add(tf.keras.layers.Conv2D(256, (3, 3), padding='same'))\n",
    "\n",
    "model.add(ResnetIdentityBlock((3,3), filters=(256, 256)))\n",
    "model.add(ResnetIdentityBlock((3,3), filters=(256, 256)))\n",
    "model.add(ResnetIdentityBlock((3,3), filters=(256, 256)))\n",
    "model.add(tf.keras.layers.Conv2D(128, (3, 3), padding='same'))\n",
    "\n",
    "\n",
    "model.add(ResnetIdentityBlock((3,3), filters=(128, 128)))\n",
    "model.add(ResnetIdentityBlock((3,3), filters=(128, 128)))\n",
    "model.add(ResnetIdentityBlock((3,3), filters=(128, 128)))\n",
    "model.add(tf.keras.layers.Conv2D(64, (3, 3), padding='same'))\n",
    "\n",
    "model.add(ResnetIdentityBlock((2,2), filters=(64, 64)))\n",
    "model.add(ResnetIdentityBlock((2,2), filters=(64, 64)))\n",
    "model.add(ResnetIdentityBlock((2,2), filters=(64, 64)))\n",
    "\n",
    "model.add(tf.keras.layers.BatchNormalization())\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.AveragePooling2D(pool_size=8))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "model.add(tf.keras.layers.Dense(128))\n",
    "model.add(tf.keras.layers.Activation('relu'))\n",
    "model.add(tf.keras.layers.Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen.fit(x_train)\n",
    "\n",
    "lr_schedule = tfa.optimizers.CyclicalLearningRate(\n",
    "    initial_learning_rate=1e-6,\n",
    "    maximal_learning_rate=1e-5,\n",
    "    step_size=2000,\n",
    "    scale_fn=lambda x: 1.,\n",
    "    scale_mode=\"cycle\",\n",
    "    name=\"MyCyclicScheduler\")\n",
    "\n",
    "adam = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "model.compile(optimizer=adam,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', patience=10, verbose=1, mode='max')\n",
    "    \n",
    "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_weights_only=True, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 1562.5 steps, validate for 78.125 steps\n",
      "Epoch 1/100\n",
      "1485/1562 [===========================>..] - ETA: 27s - loss: 207.3538 - accuracy: 0.0033WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 156250.0 batches). You may need to use the repeat() function when building your dataset.\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_accuracy` which is not available. Available metrics are: loss,accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f05700e2250>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(datagen.flow(x_train, y_train, batch_size=BATCH_SIZE, subset='training'),\n",
    "                    validation_data=datagen.flow(x_train, y_train, batch_size=BATCH_SIZE, subset='validation'),\n",
    "                    steps_per_epoch= len(x_train) / BATCH_SIZE,\n",
    "                    validation_steps = len(x_train) / BATCH_SIZE * 0.05,\n",
    "                    epochs=NUM_EPOCHS,\n",
    "                    callbacks=[es, mc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'val_accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-40804af491e0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# summarize history for accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'val_accuracy'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history['accuracy'])\n",
    "plt.plot(history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml-gpu)",
   "language": "python",
   "name": "ml-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
